{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e4110d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import requests\n",
    "from requests_html import HTMLSession\n",
    "import html5lib\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e492f39-da88-4b0a-a823-5eb19cda90dd",
   "metadata": {},
   "source": [
    "\n",
    "Scraping indeed job boards to obtain a data set which can be used for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7e359d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the connection\n",
    "\n",
    "# Indeed search for data analyst jobs, displaying 50 per page\n",
    "\n",
    "URL =\"https://www.indeed.com/jobs?as_and=data%20analyst&as_phr&as_any&as_not&as_ttl&as_cmp&jt=all&st&salary&radius=25&l&fromage=any&limit=50&sort&psf=advsrch&from=advancedsearch&vjk=44dce43f1d302551&advn=5680616533559579\"\n",
    "\n",
    "headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/99.0.4844.51 Safari/537.36 \"     \n",
    "          }  \n",
    "\n",
    "response = requests.get(URL,headers=headers)\n",
    "\n",
    "content = response.content\n",
    "\n",
    "# print(content)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ebbf003",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using beautifulsoup to parse the HTMML\n",
    "\n",
    "\n",
    "soup1=BeautifulSoup(response.content, 'html.parser')\n",
    "soup = BeautifulSoup(soup1.prettify(), 'html.parser')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3ffe57fa",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 're' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [4]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     13\u001b[0m content \u001b[38;5;241m=\u001b[39m content\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTo\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m0\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     14\u001b[0m content \u001b[38;5;241m=\u001b[39m content\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m30+\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m30\u001b[39m\u001b[38;5;124m'\u001b[39m)            \u001b[38;5;66;03m# Jobs Posted 30+ days ago as 30 days ago\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m content \u001b[38;5;241m=\u001b[39m \u001b[43mre\u001b[49m\u001b[38;5;241m.\u001b[39msub(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m^\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms+|\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms+$\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m, content, flags\u001b[38;5;241m=\u001b[39mre\u001b[38;5;241m.\u001b[39mUNICODE)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(content)\n",
      "\u001b[1;31mNameError\u001b[0m: name 're' is not defined"
     ]
    }
   ],
   "source": [
    "# Cleaning date data to extract only numbers.\n",
    "\n",
    "DatePosted = soup.find_all(\"span\", class_=\"date\")\n",
    "for dates in DatePosted:\n",
    "    content = dates.text.replace('\\n','').strip()\n",
    "    content = content.replace('Employer','')\n",
    "    content = content.replace('Posted','')\n",
    "    content = content.replace('Active','')\n",
    "    content = content.replace('ago','')\n",
    "    content = content.replace('days','')\n",
    "    content = content.replace('day','')\n",
    "    content = content.replace('Just posted','0')     # Consider jobs posted today as 0 days ago\n",
    "    content = content.replace('To','0')\n",
    "    content = content.replace('30+','30')            # Jobs Posted 30+ days ago as 30 days ago\n",
    "    content = re.sub(\"^\\s+|\\s+$\", \"\", content, flags=re.UNICODE)\n",
    "    print(content)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a1d778",
   "metadata": {},
   "outputs": [],
   "source": [
    "Company_Names=[] \n",
    "Dates_Posted = []\n",
    "Job_Titles = []\n",
    "Company_Locations=[]\n",
    "Job_Salaries = []\n",
    "\n",
    "\n",
    "# Creating lists and turning the list data into a datframe and cleaning up string and date data.\n",
    "\n",
    "\n",
    "\n",
    "CompanyNames = soup.find_all(\"span\",class_=\"companyName\")\n",
    "for name in CompanyNames:\n",
    "    content = name.text.strip().replace(\".\", \"\")\n",
    "    editedSource = re.sub('\\â€”','',str(content))\n",
    "    Company_Names.append(content)\n",
    "\n",
    "\n",
    "DatePosted = soup.find_all(\"span\", class_=\"date\")\n",
    "for dates in DatePosted:\n",
    "    content = dates.text.replace('\\n','').strip()\n",
    "    content = content.replace('Employer','')\n",
    "    content = content.replace('Posted','')\n",
    "    content = content.replace('Active','')\n",
    "    content = content.replace('ago','')\n",
    "    content = content.replace('days','')\n",
    "    content = content.replace('day','')\n",
    "    content = content.replace('Just posted','0')     # Consider jobs posted today as 0 days ago\n",
    "    content = content.replace('To','0')              # day removed, jobs posted today 'to' set as 0\n",
    "    content = content.replace('30+','30')            # Jobs Posted 30+ days ago as 30 days ago\n",
    "    content = re.sub(\"^\\s+|\\s+$\", \"\", content, flags=re.UNICODE)\n",
    "    Dates_Posted.append(content)\n",
    "    \n",
    "    \n",
    "    \n",
    "Titles = soup.find_all(\"h2\", class_=\"jobTitle\")\n",
    "for title in Titles:\n",
    "    data = title.text.replace('\\n','').strip()\n",
    "    data = data.replace('new','')\n",
    "    data = re.sub(r\"^\\s+\", \"\", data, flags=re.UNICODE)\n",
    "    data = re.sub('\\â€”','',str(data))\n",
    "    Job_Titles.append(data)\n",
    "\n",
    "\n",
    "\n",
    "CompanyLocation = soup.find_all(\"div\",class_=\"companyLocation\")\n",
    "for loca in CompanyLocation:\n",
    "    dataloc = loca.text.replace('\\n','').strip()\n",
    "    dataloc = dataloc.replace('(','')\n",
    "    dataloc = dataloc.replace(')','')\n",
    "    dataloc = dataloc.replace('+','')\n",
    "    dataloc = dataloc.replace('locations','')\n",
    "    dataloc = dataloc.replace('location','')\n",
    "    dataloc = dataloc.replace('  ','')\n",
    "    dataloc = re.sub('\\d', '', dataloc)\n",
    "    dataloc = re.sub(\"^\\s+|\\s+$\", \"\", dataloc, flags=re.UNICODE)\n",
    "    Company_Locations.append(dataloc)\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "#SalaryRange = soup.find_all(\"div\",class_=\"metadata salary-snippet-container\")\n",
    "#for sal in SalaryRange:\n",
    "#    Job_Salaries.append((sal.text.strip()))\n",
    "\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "191af975",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "df = pd.DataFrame( \n",
    "{  'Job Title' : Job_Titles,\n",
    "    'Company Name': Company_Names,\n",
    "    'Company Location': Company_Locations,\n",
    "    'Days Posted Ago': Dates_Posted,\n",
    " #   'Salary Range': Job_Salaries}\n",
    "}, columns=[\"Job Title\", \"Company Name\", \"Company Location\", \"Days Posted Ago\"]\n",
    ")\n",
    "\n",
    "\n",
    "print(df)\n",
    "\n",
    "df.to_csv('IndeedScraping.csv', index=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c03a9882",
   "metadata": {},
   "outputs": [],
   "source": [
    " # Function to automate this process - \n",
    "\n",
    "def scrap_indeed():\n",
    "    import datetime\n",
    "    import csv\n",
    "    from bs4 import BeautifulSoup\n",
    "    import requests\n",
    "    import requests\n",
    "    from requests_html import HTMLSession\n",
    "    import html5lib\n",
    "    import pandas as pd\n",
    "    import re\n",
    "    \n",
    "    URL =\"https://www.indeed.com/jobs?as_and=data%20analyst&as_phr&as_any&as_not&as_ttl&as_cmp&jt=all&st&salary&radius=25&l&fromage=any&limit=50&sort&psf=advsrch&from=advancedsearch&vjk=44dce43f1d302551&advn=5680616533559579\"\n",
    "\n",
    "    headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/99.0.4844.51 Safari/537.36 \"     \n",
    "          }  \n",
    "\n",
    "    response = requests.get(URL,headers=headers)\n",
    "    content = response.content\n",
    "    soup1=BeautifulSoup(response.content, 'html.parser')\n",
    "    soup = BeautifulSoup(soup1.prettify(), 'html.parser')\n",
    "\n",
    "    Company_Names=[] \n",
    "    Dates_Posted = []\n",
    "    Job_Titles = []\n",
    "    Company_Locations=[]\n",
    "    Job_Salaries = []\n",
    "\n",
    "\n",
    "    CompanyNames = soup.find_all(\"span\",class_=\"companyName\")\n",
    "    for name in CompanyNames:\n",
    "        content = name.text.strip().replace(\".\", \"\")\n",
    "        editedSource = re.sub('\\â€”','',str(content))\n",
    "        Company_Names.append(content)\n",
    "\n",
    "\n",
    "    DatePosted = soup.find_all(\"span\", class_=\"date\")\n",
    "    for dates in DatePosted:\n",
    "        content = dates.text.replace('\\n','').strip()\n",
    "        content = content.replace('Employer','')\n",
    "        content = content.replace('Posted','')\n",
    "        content = content.replace('Active','')\n",
    "        content = content.replace('ago','')\n",
    "        content = content.replace('days','')\n",
    "        content = content.replace('day','')\n",
    "        content = content.replace('Just posted','0')     # Consider jobs posted today as 0 days ago\n",
    "        content = content.replace('To','0')              # day removed, jobs posted today 'to' set as 0\n",
    "        content = content.replace('30+','30')            # Jobs Posted 30+ days ago as 30 days ago\n",
    "        content = re.sub(\"^\\s+|\\s+$\", \"\", content, flags=re.UNICODE)\n",
    "        Dates_Posted.append(content)\n",
    "    \n",
    "    \n",
    "    \n",
    "    Titles = soup.find_all(\"h2\", class_=\"jobTitle\")\n",
    "    for title in Titles:\n",
    "        data = title.text.replace('\\n','').strip()\n",
    "        data = data.replace('new','')\n",
    "        data = re.sub(r\"^\\s+\", \"\", data, flags=re.UNICODE)\n",
    "        data = re.sub('\\â€”','',str(data))\n",
    "        Job_Titles.append(data)\n",
    "\n",
    "\n",
    "\n",
    "    CompanyLocation = soup.find_all(\"div\",class_=\"companyLocation\")\n",
    "    for loca in CompanyLocation:\n",
    "        dataloc = loca.text.replace('\\n','').strip()\n",
    "        dataloc = dataloc.replace('(','')\n",
    "        dataloc = dataloc.replace(')','')\n",
    "        dataloc = dataloc.replace('+','')\n",
    "        dataloc = dataloc.replace('locations','')\n",
    "        dataloc = dataloc.replace('location','')\n",
    "        dataloc = dataloc.replace('  ','')\n",
    "        dataloc = re.sub('\\d', '', dataloc)\n",
    "        dataloc = re.sub(\"^\\s+|\\s+$\", \"\", dataloc, flags=re.UNICODE)\n",
    "        Company_Locations.append(dataloc)\n",
    "    \n",
    "    \n",
    "    df = pd.DataFrame( \n",
    "    {  'Job Title' : Job_Titles,\n",
    "        'Company Name': Company_Names,\n",
    "        'Company Location': Company_Locations,\n",
    "        'Days Posted Ago': Dates_Posted,}\n",
    "    , columns=[\"Job Title\", \"Company Name\", \"Company Location\", \"Days Posted Ago\"]\n",
    "    )\n",
    "\n",
    "    \n",
    "    with open('IndeedScraping.csv', 'a+', newline='', encoding='UTF8') as f:\n",
    "        writer=csv.writer(f)\n",
    "        writer.writerow(data)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bdb5009",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "while(True):\n",
    "    scrap_indeed()\n",
    "    time.sleep(1000)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c7a25fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89238cff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
